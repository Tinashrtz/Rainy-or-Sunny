{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.spatial.distance import euclidean\n",
    "from sklearn.feature_selection import SequentialFeatureSelector\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0  2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1  2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2  2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3  2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4  2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Humidity9am  Humidity3pm  \\\n",
      "0           W           44.0          W  ...        71.0         22.0   \n",
      "1         WNW           44.0        NNW  ...        44.0         25.0   \n",
      "2         WSW           46.0          W  ...        38.0         30.0   \n",
      "3          NE           24.0         SE  ...        45.0         16.0   \n",
      "4           W           41.0        ENE  ...        82.0         33.0   \n",
      "\n",
      "   Pressure9am  Pressure3pm  Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  \\\n",
      "0       1007.7       1007.1       8.0       NaN     16.9     21.8         No   \n",
      "1       1010.6       1007.8       NaN       NaN     17.2     24.3         No   \n",
      "2       1007.6       1008.7       NaN       2.0     21.0     23.2         No   \n",
      "3       1017.6       1012.8       NaN       NaN     18.1     26.5         No   \n",
      "4       1010.8       1006.0       7.0       8.0     17.8     29.7         No   \n",
      "\n",
      "   RainTomorrow  \n",
      "0            No  \n",
      "1            No  \n",
      "2            No  \n",
      "3            No  \n",
      "4            No  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Dataset shape: (145460, 23)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"BOM.csv\")\n",
    "print(df.head())\n",
    "dataset_shape = df.shape\n",
    "print(f\"Dataset shape: {dataset_shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "remove invalid targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (142193, 23)\n",
      "(142193, 23)\n"
     ]
    }
   ],
   "source": [
    "df_filtered = df[df['RainTomorrow'].isin(['Yes', 'No'])]\n",
    "dataset_shape = df_filtered.shape\n",
    "print(f\"Dataset shape: {dataset_shape}\")\n",
    "df_filtered.to_csv(\"cleaned_target.csv\", index=False)\n",
    "df = pd.read_csv(\"cleaned_target.csv\")\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " در این بخش همانطور که در صورت سوال خواسته شده، داده هایی را که در هدف مقداری غیر از بله یا خیی داشتند،حذف کرده ایم.همچنین در اخر اندازه دیتافریم را بررس کردم تا مطمين شوم عملیات حذف اتفاق افتاده است. همچنین دیتافریم بدست آمده را در یک فایل جدید ذخیره کردم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add month to data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Date Location  MinTemp  MaxTemp  Rainfall  Evaporation  Sunshine  \\\n",
      "0 2008-12-01   Albury     13.4     22.9       0.6          NaN       NaN   \n",
      "1 2008-12-02   Albury      7.4     25.1       0.0          NaN       NaN   \n",
      "2 2008-12-03   Albury     12.9     25.7       0.0          NaN       NaN   \n",
      "3 2008-12-04   Albury      9.2     28.0       0.0          NaN       NaN   \n",
      "4 2008-12-05   Albury     17.5     32.3       1.0          NaN       NaN   \n",
      "\n",
      "  WindGustDir  WindGustSpeed WindDir9am  ... Pressure9am  Pressure3pm  \\\n",
      "0           W           44.0          W  ...      1007.7       1007.1   \n",
      "1         WNW           44.0        NNW  ...      1010.6       1007.8   \n",
      "2         WSW           46.0          W  ...      1007.6       1008.7   \n",
      "3          NE           24.0         SE  ...      1017.6       1012.8   \n",
      "4           W           41.0        ENE  ...      1010.8       1006.0   \n",
      "\n",
      "   Cloud9am  Cloud3pm  Temp9am  Temp3pm  RainToday  RainTomorrow  Year  Month  \n",
      "0       8.0       NaN     16.9     21.8         No            No  2008     12  \n",
      "1       NaN       NaN     17.2     24.3         No            No  2008     12  \n",
      "2       NaN       2.0     21.0     23.2         No            No  2008     12  \n",
      "3       NaN       NaN     18.1     26.5         No            No  2008     12  \n",
      "4       7.0       8.0     17.8     29.7         No            No  2008     12  \n",
      "\n",
      "[5 rows x 25 columns]\n",
      "(142193, 25)\n"
     ]
    }
   ],
   "source": [
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df['Year'] = df['Date'].dt.year\n",
    "df['Month'] = df['Date'].dt.month\n",
    "print(df.head())\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش از تاریخ های موجود در دیتاست اصتفاده کردم و ماه و سال را از آنها استخراج کردم. این کار برای ادامه کد که میخواهیم از میانگین و مد برای داده های خالی استفاده کنیم نیاز است"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Median and Mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142193, 23)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#columns with numerical and object values\n",
    "numeric_columns = df.select_dtypes(include=['float64', 'int64']).columns\n",
    "object_columns = df.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Function to replace missing values in numerical columns with the median of the corresponding month\n",
    "def fill_numeric_with_monthly_median(dataframe, columns):\n",
    "    for column in columns:\n",
    "        # Compute the median for each year-month group\n",
    "        medians = dataframe.groupby(['Year', 'Month'])[column].transform('median')\n",
    "        # Replace empty strings with NaN if they exist\n",
    "        dataframe[column].replace(\"\", pd.NA, inplace=True)\n",
    "        # Fill missing values with the corresponding median\n",
    "        dataframe[column].fillna(medians, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# Function to replace missing values in object columns with the mode (most frequent value) of the corresponding month\n",
    "def fill_object_with_monthly_mode(dataframe, columns):\n",
    "    for column in columns:\n",
    "        # Compute the mode for each year-month group\n",
    "        modes = dataframe.groupby(['Year', 'Month'])[column].transform(lambda x: x.mode()[0] if not x.mode().empty else x)\n",
    "        # Replace empty strings with NaN if they exist\n",
    "        dataframe[column].replace(\"\", pd.NA, inplace=True)\n",
    "        # Fill missing values with the corresponding mode\n",
    "        dataframe[column].fillna(modes, inplace=True)\n",
    "    return dataframe\n",
    "\n",
    "# Replace missing values with the median for numerical columns\n",
    "df = fill_numeric_with_monthly_median(df, numeric_columns)\n",
    "\n",
    "# Replace missing values with the mode for object columns\n",
    "df = fill_object_with_monthly_mode(df, object_columns)\n",
    "\n",
    "# Drop the 'Year' and 'Month' columns as they were temporary for the calculation\n",
    "df.drop(columns=['Year', 'Month'], inplace=True)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.shape)\n",
    "# Save the cleaned DataFrame back to a CSV file (optional)\n",
    "df.to_csv('NotNull_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش داده های عددی و رشته ای را جداگانه بررسی کردم . داده های عددی ای که مقدار نداشتند را با میانگین داده های دیگر در همان ماه پر کردم . داده های رشته ای که مقدار نداشتند را با ماکسیمم داده تکرار شده در همان ماه یا همان مد داده ها ، پر کردم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduce the number of variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/n23dh2ds1wn5534zd6s5hkfm0000gn/T/ipykernel_10802/3906929220.py:2: FutureWarning: The default value of numeric_only in DataFrame.corr is deprecated. In a future version, it will default to False. Select only valid columns or specify the value of numeric_only to silence this warning.\n",
      "  correlation_matrix = df.corr().abs()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Highly Correlated Feature Pairs:\n",
      "Location - Pressure9am : Correlation = 0.9716302291283201\n",
      "WindDir3pm - WindSpeed9am : Correlation = 0.9611560115076542\n",
      "Removed features: ['Pressure9am', 'WindSpeed9am']\n",
      "(142193, 21)\n"
     ]
    }
   ],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr().abs()\n",
    "\n",
    "# Get the upper triangle of the correlation matrix\n",
    "upper_triangle = correlation_matrix.where(np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool))\n",
    "\n",
    "# Find feature pairs with correlation coefficients above 0.95\n",
    "highly_correlated_pairs = [(i, j) for i in range(len(upper_triangle.columns)) for j in range(i+1, len(upper_triangle.columns)) if upper_triangle.iloc[i, j] > 0.95]\n",
    "\n",
    "# Print highly correlated feature pairs\n",
    "print(\"Highly Correlated Feature Pairs:\")\n",
    "for pair in highly_correlated_pairs:\n",
    "    print(df.columns[pair[0]], \"-\", df.columns[pair[1]], \": Correlation =\", upper_triangle.iloc[pair[0], pair[1]])\n",
    "    \n",
    "removed_features = []\n",
    "for pair in highly_correlated_pairs:\n",
    "    removed_features.append(df.columns[pair[1]])\n",
    "    df.drop(columns=[df.columns[pair[1]]], inplace=True)\n",
    "print(\"Removed features:\", removed_features)\n",
    "\n",
    "print(df.shape)\n",
    "df.to_csv('reduced_dataset.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش میخواهیم برای اینکه بتوانیم مدل بهتری ترین کنیم ، تعداد فیچر هارا کم کنیم. یک راه حل این است که میزان همبستگی هر دو فیچر متمایز از همدیگر را بدست آوریم. در صورت سوال گفته شده اگر کوریلیشن بالاتر از ۰.۹۵ بود از هر پیر یکی از فیچر ها حذف شود که در اینجا دو زوج ویژگی با کوریلیشن بیشتر از ۰.۹۵ بدست آوردیم و به صورت دلخواه از هر کدام دومین ویژگی را حذف کردیم. به \n",
    "کمک این روش توانستیم تعداد فیچر هارا کم کنیم\n",
    "\n",
    "\n",
    "----------------------------------------------------------------------------------------------------\n",
    "problem 3 - question A - answer:\n",
    "\n",
    "همچنین در انتهای سوال مطرح شده است که آیا نگهداشتن دو فیچر در دیتاست که همبستگی بسیار بالایی با یکدیگر دارند کار درست یا خیر ؟\n",
    "   از نظر من این کار درست نیست زیرا میتواند مشکلات زیر را ایجاد کند\n",
    "   برازش بیش‌ازحد: گنجاندن ویژگی‌های بسیار مرتبط در یک مدل می‌تواند منجر به برازش بیش از حد شود، که در آن مدل نویز را در داده‌های آموزشی به جای روابط زیربنایی یاد می‌گیرد\n",
    "\n",
    "   ناپایداری تخمین های ضرایب: چند خطی بودن می تواند باعث شود که تخمین ضرایب ناپایدار باشد و نسبت به تغییرات کوچک در داده ها بسیار حساس باشد\n",
    "\n",
    "   تفسیرپذیری کاهش یافته: تفسیر اثرات فردی ویژگی های همبسته روی متغیر هدف زمانی که آنها به شدت هم خطی هستند دشوار می شود\n",
    "\n",
    "   بنابراین بهتر است یکی از این زوج ویژگی هارا حذف کنیم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(142193, 20)\n",
      "(142193, 19)\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('reduced_dataset.csv')\n",
    "\n",
    "# Drop any columns that are not useful or cannot be converted to numerical format\n",
    "# For example, dropping 'Date' column\n",
    "df.drop(columns=['Date'], inplace=True)\n",
    "print(df.shape)\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(columns=['RainTomorrow'])  \n",
    "y = df['RainTomorrow']\n",
    "print(X.shape)\n",
    "\n",
    "# Initialize LabelEncoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to target variable (boolean)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Apply label encoding to categorical features\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while encoding column '{col}': {e}\")\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "X['Target'] = y\n",
    "X.to_csv('encoded_data.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش از یک روش انکدینگ استفاده کردیم تا داده های استرینگ را به عدد تبدیل کنیم تا در ادامه برای ترین کردن مدل ها به مشکل نخوریم\n",
    "من سعی کردم بدون انکدینگ هم مدل را پیاده سازی کنم اما تنها راه ممکن حذف کردن داده های غیر عددی بود که پس از انجام این کار دقت مدل به شدت پایین تر از دقت فعلی بود . برای همین تصمیم گرفتم به جای حذف داد ها از انکد کردن آنها استفاده کنم"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10 best features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The 10 best features selected by Sequential Feature Selection:\n",
      "Location\n",
      "MinTemp\n",
      "MaxTemp\n",
      "Sunshine\n",
      "WindGustSpeed\n",
      "WindSpeed3pm\n",
      "Humidity9am\n",
      "Humidity3pm\n",
      "Pressure3pm\n",
      "Temp9am\n",
      "\n",
      "Train set size: 113754\n",
      "Validation set size: 14219\n",
      "Test set size: 14220\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('encoded_data.csv')\n",
    "X = df.drop(columns=['Target'])  \n",
    "y = df['Target']\n",
    "# Split the data into 'train', 'validation', and 'test' sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Initialize a random forest classifier with default hyperparameters\n",
    "clf = RandomForestClassifier(n_estimators=5, max_depth=10, random_state=42)\n",
    "\n",
    "# Initialize Sequential Feature Selector\n",
    "sfs = SequentialFeatureSelector(clf, n_features_to_select=10, direction='forward', scoring='accuracy', cv=4)\n",
    "\n",
    "# Fit the Sequential Feature Selector to the training data\n",
    "sfs.fit(X_train, y_train)\n",
    "\n",
    "# Get the names of the selected features\n",
    "selected_features = X_train.columns[sfs.support_]\n",
    "\n",
    "# Print the selected features\n",
    "print(\"The 10 best features selected by Sequential Feature Selection:\")\n",
    "for feature in selected_features:\n",
    "    print(feature)\n",
    "    \n",
    "# Assuming 'important_features' contains the names of the ten most important features\n",
    "important_features = selected_features\n",
    "\n",
    "\n",
    "\n",
    "# Create a new DataFrame with only the important features and the target column\n",
    "new_df = df[important_features].copy()\n",
    "new_df['Target'] = y\n",
    "new_df.to_csv('important.csv', index=False)\n",
    "\n",
    "\n",
    "# Optionally, print the sizes of the resulting sets\n",
    "print(\"\\nTrain set size:\", X_train.shape[0])\n",
    "print(\"Validation set size:\", X_validation.shape[0])\n",
    "print(\"Test set size:\", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش به کمک کتابخانه رندوم فارست ۱۰ فیچر با اهمیت بیشتر را انتخاب کردم تا مدل بعدی را بر روی دیتاستی با همین ۱۰ ویژگی ترین کنم. این کار قطعا باعث کم شدن دقت نسبت به حالتی که مدل را بر روی همه ویژگی ها ترین کنیم، میشود. اما باعث بالاتر رفتن  سرعت ران شدن کد، میشود. همچنین من کد را بدون حذف ویژگی ها با کمک گوگل کولب ران کردم و دقت بدست آمده با دقت حالتی که در ادامه بررسی میکنیم تفاوت زیادی نداشت "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy for k=1: 0.7935860468387369\n",
      "Validation accuracy for k=2: 0.8212251213165482\n",
      "Validation accuracy for k=3: 0.8250228567409804\n",
      "Validation accuracy for k=4: 0.8321963569871299\n",
      "Validation accuracy for k=5: 0.8338842393979886\n",
      "Validation accuracy for k=6: 0.8364160630142766\n",
      "Validation accuracy for k=7: 0.8371896757859203\n",
      "Validation accuracy for k=8: 0.8417610239819959\n",
      "Validation accuracy for k=9: 0.8411983965117097\n",
      "Validation accuracy for k=10: 0.841128068077924\n",
      "Validation accuracy for k=11: 0.8409874112103524\n",
      "Validation accuracy for k=12: 0.8421829945847106\n",
      "Validation accuracy for k=13: 0.8426049651874252\n",
      "Validation accuracy for k=14: 0.8426049651874252\n",
      "Validation accuracy for k=15: 0.8426049651874252\n",
      "Validation accuracy for k=16: 0.8452071172374991\n",
      "Validation accuracy for k=17: 0.8440115338631409\n",
      "Validation accuracy for k=18: 0.8442928475982839\n",
      "Validation accuracy for k=19: 0.8445038328996414\n",
      "Validation accuracy for k=20: 0.8440115338631409\n",
      "Validation accuracy for k=21: 0.8443631760320698\n",
      "Validation accuracy for k=22: 0.8454884309726423\n",
      "Validation accuracy for k=23: 0.8450664603699276\n",
      "Validation accuracy for k=24: 0.8441521907307125\n",
      "Validation accuracy for k=25: 0.8442928475982839\n",
      "Validation accuracy for k=26: 0.8459807300091426\n",
      "Validation accuracy for k=27: 0.8452071172374991\n",
      "Validation accuracy for k=28: 0.8454181025388564\n",
      "Validation accuracy for k=29: 0.8440115338631409\n",
      "Validation accuracy for k=30: 0.8443631760320698\n",
      "Validation accuracy for k=31: 0.8443631760320698\n",
      "Validation accuracy for k=32: 0.8435895632604262\n",
      "Validation accuracy for k=33: 0.8426049651874252\n",
      "Validation accuracy for k=34: 0.8435895632604262\n",
      "Validation accuracy for k=35: 0.8428862789225684\n",
      "Validation accuracy for k=36: 0.8433785779590689\n",
      "Validation accuracy for k=37: 0.8430972642239257\n",
      "Validation accuracy for k=38: 0.8434489063928546\n",
      "Validation accuracy for k=39: 0.8435192348266404\n",
      "Validation accuracy for k=40: 0.8434489063928546\n",
      "Validation accuracy for k=41: 0.8440818622969266\n",
      "Validation accuracy for k=42: 0.8435192348266404\n",
      "Validation accuracy for k=43: 0.8434489063928546\n",
      "Validation accuracy for k=44: 0.8427456220549968\n",
      "Validation accuracy for k=45: 0.8432379210914973\n",
      "Validation accuracy for k=46: 0.8434489063928546\n",
      "Validation accuracy for k=47: 0.8437302201279977\n",
      "Validation accuracy for k=48: 0.8432379210914973\n",
      "Validation accuracy for k=49: 0.8431675926577115\n",
      "Validation accuracy for k=50: 0.8435895632604262\n",
      "Validation accuracy for k=51: 0.8435895632604262\n",
      "Validation accuracy for k=52: 0.8435895632604262\n",
      "Validation accuracy for k=53: 0.8435192348266404\n",
      "Validation accuracy for k=54: 0.8425346367536395\n",
      "Validation accuracy for k=55: 0.8429566073563541\n",
      "Validation accuracy for k=56: 0.8429566073563541\n",
      "Validation accuracy for k=57: 0.843659891694212\n",
      "Validation accuracy for k=58: 0.843308249525283\n",
      "Validation accuracy for k=59: 0.8432379210914973\n",
      "Validation accuracy for k=60: 0.8434489063928546\n",
      "Validation accuracy for k=61: 0.8435192348266404\n",
      "Validation accuracy for k=62: 0.8431675926577115\n",
      "Validation accuracy for k=63: 0.8428862789225684\n",
      "Validation accuracy for k=64: 0.8423236514522822\n",
      "Validation accuracy for k=65: 0.8424643083198537\n",
      "Validation accuracy for k=66: 0.8426049651874252\n",
      "Validation accuracy for k=67: 0.8432379210914973\n",
      "Validation accuracy for k=68: 0.8429566073563541\n",
      "Validation accuracy for k=69: 0.843308249525283\n",
      "Validation accuracy for k=70: 0.8428159504887827\n",
      "Validation accuracy for k=71: 0.8431675926577115\n",
      "Validation accuracy for k=72: 0.8428862789225684\n",
      "Validation accuracy for k=73: 0.8435895632604262\n",
      "Validation accuracy for k=74: 0.84302693579014\n",
      "Validation accuracy for k=75: 0.8432379210914973\n",
      "Validation accuracy for k=76: 0.8434489063928546\n",
      "Validation accuracy for k=77: 0.8431675926577115\n",
      "Validation accuracy for k=78: 0.843308249525283\n",
      "Validation accuracy for k=79: 0.8438005485617835\n",
      "Validation accuracy for k=80: 0.8433785779590689\n",
      "Validation accuracy for k=81: 0.8441521907307125\n",
      "Validation accuracy for k=82: 0.8434489063928546\n",
      "Validation accuracy for k=83: 0.8437302201279977\n",
      "Validation accuracy for k=84: 0.8428159504887827\n",
      "Validation accuracy for k=85: 0.8428159504887827\n",
      "Validation accuracy for k=86: 0.8422533230184964\n",
      "Validation accuracy for k=87: 0.8424643083198537\n",
      "Validation accuracy for k=88: 0.842042337717139\n",
      "Validation accuracy for k=89: 0.8427456220549968\n",
      "Validation accuracy for k=90: 0.8426752936212111\n",
      "Validation accuracy for k=91: 0.8428862789225684\n",
      "Validation accuracy for k=92: 0.8426049651874252\n",
      "Validation accuracy for k=93: 0.8432379210914973\n",
      "Validation accuracy for k=94: 0.8428862789225684\n",
      "Validation accuracy for k=95: 0.8434489063928546\n",
      "Validation accuracy for k=96: 0.8430972642239257\n",
      "Validation accuracy for k=97: 0.84302693579014\n",
      "Validation accuracy for k=98: 0.8433785779590689\n",
      "Validation accuracy for k=99: 0.8430972642239257\n",
      "\n",
      "Best value of k: 26 (Validation accuracy: 0.8459807300091426)\n",
      "\n",
      "Accuracy on the test set: 0.8457102672292546\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('important.csv')\n",
    "X = df.drop(columns=['Target']) \n",
    "y = df['Target']\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "X_validation, X_test, y_validation, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "\n",
    "best_accuracy = 0\n",
    "best_k = None\n",
    "\n",
    "# Iterate over different values of k\n",
    "for k in range(1,100):\n",
    "    # Initialize K-NN classifier with the current value of k\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    \n",
    "    # Train the K-NN classifier on the training set\n",
    "    knn.fit(X_train, y_train)\n",
    "    \n",
    "    # Predict labels for the validation set\n",
    "    y_pred = knn.predict(X_validation)\n",
    "    \n",
    "    # Calculate accuracy on the validation set\n",
    "    accuracy = accuracy_score(y_validation, y_pred)\n",
    "    \n",
    "    # Print the accuracy for the current value of k\n",
    "    print(f'Validation accuracy for k={k}: {accuracy}')\n",
    "    \n",
    "    # Check if this k value gives better accuracy than the previous best\n",
    "    if accuracy > best_accuracy:\n",
    "        best_accuracy = accuracy\n",
    "        best_k = k\n",
    "\n",
    "# Select the value of k that gives the best performance on the validation set\n",
    "print(f'\\nBest value of k: {best_k} (Validation accuracy: {best_accuracy})')\n",
    "\n",
    "# Train the selected K-NN model with the best value of k using only the training set\n",
    "best_knn = KNeighborsClassifier(n_neighbors=best_k)\n",
    "best_knn.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the performance of the selected model on the test set\n",
    "y_test_pred = best_knn.predict(X_test)\n",
    "test_accuracy = accuracy_score(y_test, y_test_pred)\n",
    "print(f'\\nAccuracy on the test set: {test_accuracy}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در اینجا دفت های مختلف الگوریتم در ازای مقادیر مختلف پارامتر کا بدست آمد . بر اساس بهترین کا، بهترین مدل را ترین میکنیم که دقت این مدل در خروجی بالا قابل مشاهده است"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------------------------------------------------------------\n",
    "problem 3- question e - answer :\n",
    "\n",
    "yes, it is possible to determine the probability of rainfall using a similar strategy as outlined in the provided code. Here's how you can modify the approach to predict the probability of rainfall:\n",
    "\n",
    "Model Selection: Choose a model that can provide probability estimates directly. Logistic Regression is a common choice for binary classification tasks where the output can be interpreted as the probability of the positive class (rainfall). Alternatively, ensemble methods like Random Forests and Gradient Boosting Machines (e.g., XGBoost, LightGBM) can also provide probability estimates when configured appropriately.\n",
    "\n",
    "Training and Prediction: Train the selected model on the training data as before. However, during prediction, instead of using predict() method, use predict_proba() method to obtain the probability estimates for each class (in this case, probability of rainfall and probability of no rainfall).\n",
    "\n",
    "Evaluation: Evaluate the model's performance using appropriate metrics for probability estimates. Common metrics include log loss (or cross-entropy loss), area under the receiver operating characteristic curve (ROC-AUC), and calibration plots.\n",
    "\n",
    "Calibration: Optionally, you can calibrate the predicted probabilities to improve their reliability. Calibration ensures that the predicted probabilities are well-calibrated and reflect the true likelihood of rainfall. Techniques such as Platt scaling or Isotonic regression can be used for calibration.\n",
    "\n",
    "Threshold Selection: Determine an appropriate threshold for converting probability estimates into binary predictions (rainfall or no rainfall). This threshold can be chosen based on the specific requirements of the application, such as minimizing false positives or false negatives.\n",
    "\n",
    "By following these steps, you can adapt the provided strategy to predict the probability of rainfall rather than just the binary outcome. This allows for a more nuanced understanding of the likelihood of rainfall based on the given features and can be useful for various applications such as risk assessment, resource planning, and decision-making.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RASHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 22)\n",
      "(2, 11)\n",
      "(2, 10)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "<Flags(allows_duplicate_labels=True)>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/5l/n23dh2ds1wn5534zd6s5hkfm0000gn/T/ipykernel_10802/3952886121.py:39: FutureWarning: Passing a set as an indexer is deprecated and will raise in a future version. Use a list instead.\n",
      "  df_selected = df[common_features]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Flags' object has no attribute 'c_contiguous'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 50\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(X\u001b[38;5;241m.\u001b[39mflags)\n\u001b[1;32m     49\u001b[0m best_knn\u001b[38;5;241m.\u001b[39mfit(X,y)\n\u001b[0;32m---> 50\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m best_knn\u001b[38;5;241m.\u001b[39mpredict(X)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Calculate and print the accuracy on the test set\u001b[39;00m\n\u001b[1;32m     54\u001b[0m report \u001b[38;5;241m=\u001b[39m classification_report(y, y_pred)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_classification.py:246\u001b[0m, in \u001b[0;36mKNeighborsClassifier.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    244\u001b[0m check_is_fitted(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_fit_method\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 246\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_method \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbrute\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m ArgKminClassMode\u001b[38;5;241m.\u001b[39mis_usable_for(\n\u001b[1;32m    247\u001b[0m         X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric\n\u001b[1;32m    248\u001b[0m     ):\n\u001b[1;32m    249\u001b[0m         probabilities \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_proba(X)\n\u001b[1;32m    250\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs_2d_:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:471\u001b[0m, in \u001b[0;36mArgKminClassMode.is_usable_for\u001b[0;34m(cls, X, Y, metric)\u001b[0m\n\u001b[1;32m    448\u001b[0m \u001b[38;5;129m@classmethod\u001b[39m\n\u001b[1;32m    449\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_usable_for\u001b[39m(\u001b[38;5;28mcls\u001b[39m, X, Y, metric) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m    450\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return True if the dispatcher can be used for the given parameters.\u001b[39;00m\n\u001b[1;32m    451\u001b[0m \n\u001b[1;32m    452\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    468\u001b[0m \u001b[38;5;124;03m    True if the PairwiseDistancesReduction can be used, else False.\u001b[39;00m\n\u001b[1;32m    469\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    470\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[0;32m--> 471\u001b[0m         ArgKmin\u001b[38;5;241m.\u001b[39mis_usable_for(X, Y, metric)\n\u001b[1;32m    472\u001b[0m         \u001b[38;5;66;03m# TODO: Support CSR matrices.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(X)\n\u001b[1;32m    474\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m issparse(Y)\n\u001b[1;32m    475\u001b[0m         \u001b[38;5;66;03m# TODO: implement Euclidean specialization with GEMM.\u001b[39;00m\n\u001b[1;32m    476\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqeuclidean\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    477\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:115\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for\u001b[0;34m(cls, X, Y, metric)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_valid_sparse_matrix\u001b[39m(X):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m    103\u001b[0m         isspmatrix_csr(X)\n\u001b[1;32m    104\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    110\u001b[0m         X\u001b[38;5;241m.\u001b[39mindices\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m X\u001b[38;5;241m.\u001b[39mindptr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mint32\n\u001b[1;32m    111\u001b[0m     )\n\u001b[1;32m    113\u001b[0m is_usable \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    114\u001b[0m     get_config()\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menable_cython_pairwise_dist\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(X) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(X))\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (is_numpy_c_ordered(Y) \u001b[38;5;129;01mor\u001b[39;00m is_valid_sparse_matrix(Y))\n\u001b[1;32m    117\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m Y\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m    118\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;129;01min\u001b[39;00m (np\u001b[38;5;241m.\u001b[39mfloat32, np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m    119\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m metric \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mvalid_metrics()\n\u001b[1;32m    120\u001b[0m )\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_usable\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/metrics/_pairwise_distances_reduction/_dispatcher.py:99\u001b[0m, in \u001b[0;36mBaseDistancesReductionDispatcher.is_usable_for.<locals>.is_numpy_c_ordered\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mis_numpy_c_ordered\u001b[39m(X):\n\u001b[0;32m---> 99\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflags\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m X\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Flags' object has no attribute 'c_contiguous'"
     ]
    }
   ],
   "source": [
    "Rasht_data = pd.read_csv(\"Rasht.csv\")\n",
    "Rasht_data['Date'] = pd.to_datetime(Rasht_data['Date'])\n",
    "Rasht_data['Month'] = Rasht_data['Date'].dt.month\n",
    "Rasht_data = Rasht_data.drop(columns=[\"Date\"])\n",
    "\n",
    "X = Rasht_data.drop(columns=['RainTomorrow'])  \n",
    "y = Rasht_data['RainTomorrow']\n",
    "print(X.shape)\n",
    "\n",
    "# Initialize LabelEncoder for target variable\n",
    "label_encoder = LabelEncoder()\n",
    "\n",
    "# Apply label encoding to target variable (boolean)\n",
    "y = label_encoder.fit_transform(y)\n",
    "\n",
    "# Apply label encoding to categorical features\n",
    "for col in X.select_dtypes(include=['object']).columns:\n",
    "    try:\n",
    "        X[col] = label_encoder.fit_transform(X[col])\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while encoding column '{col}': {e}\")\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "X['Target'] = y\n",
    "X.to_csv('rasht_encoded.csv', index=False)\n",
    "\n",
    "\n",
    "# KNN\n",
    "df = pd.read_csv('rasht_encoded.csv')\n",
    "df_2=pd.read_csv('important.csv')\n",
    "# Extract feature names from both datasets\n",
    "features_A = set(df.columns)\n",
    "features_B = set(df_2.columns)\n",
    "\n",
    "# Find common features between dataset A and dataset B\n",
    "common_features = features_A.intersection(features_B)\n",
    "\n",
    "# Select only the common features from dataset B\n",
    "df_selected = df[common_features]\n",
    "print(df_selected.shape)\n",
    "\n",
    "X = df_selected.drop(columns=['Target']) \n",
    "y = df_selected['Target']\n",
    "print(X.shape)\n",
    "print(type(X))\n",
    "print(X.flags)\n",
    "\n",
    "\n",
    "best_knn.fit(X,y)\n",
    "y_pred = best_knn.predict(X)\n",
    "\n",
    "\n",
    "# Calculate and print the accuracy on the test set\n",
    "report = classification_report(y, y_pred)\n",
    "print(report)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "در این بخش دیتاست شهر رشت به صورت دستی ساخته شد و سعی کردم بر روی داده های آن به کمک بهترین الگوریتمی که در بخش قبل بدست آورده بودم ، ترینینگ انجام دهم. اما متاسفانه به دلیل تفاوت ابعادی این اتفاق نیافتاد و این بخش کد به نتیجه نرسید. اما اگر بخواهیم تحیلیل کنیم میتوانم به صورت زیر استدلال کنم :\n",
    "بهترین مدلی که در بخش قبل بدست آوردیم پس از پر کردن داده های خالی بر اساس میانگین و مد داده های دیتاست قبلی بود. همچنین از انکدری استفاده شد که باز هم مختص داده های قبلی بود. همچنی از برای حذف فیچز ها و کاهش بعد از ماتریس کوریلیشنی استفاده کردیم که بازهم مختص داده های قبلی بود. یعنی برای داده های فعلی شاید بهتر باشد فیچر های بهتری حذف شوند تا به بهترین نتیجه برسیم. همچنین بهترین مدلی که ما در حالت قبل بدست آوردیم بر اساس ۱۰ ویژگی مهم دیتاست قبلی بود که ممکن است در دیتاست شهر رشت اهمیت زیادی نداشته باشند. برای مثال در تمامی سطر ها مقادیر ثابت داشته باشند یا اصلا جزو ده ویژگی مهم دیتاست شهر رشت نباشند. بنابر دلایل ذکر شده لزومی ندارد که بهترین مدلی که در حالت ثبل بدست آوردیم بر روی دیتاست شهر رشت نیز نتیجه خوبی داشته باشد و نمیتوانیم انتظار دقت بالایی از آن داشته باشیم"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
